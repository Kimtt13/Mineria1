{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf159b5",
   "metadata": {},
   "source": [
    "# Selección de variables en regresión lineal\n",
    "\n",
    "En esta sección se exploran los métodos clásicos de selección automática de variables. \n",
    "\n",
    "## Preliminares\n",
    "\n",
    "En este documento se presentan varias alternativas para las selección automática de variables en modelos de regresión. Esta técnicas automáticas resulta útiles cuando nos enfrentamos a gran cantidad de variables y esto hace que el proceso manual sea difícil de abordar. En cualquier caso, hemos de saber que no son mágicas y que tienen sus debilidades, por lo que el control de las mismas por nuestra parte se hace fundamental de cara a la obtención de buenos resultados en su aplicación. \n",
    "\n",
    "\n",
    "Procedemos a la lectura de los datos depurados y con las transformaciones creadas en el código de regresión lineal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e4119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4998 entries, 0 to 6364\n",
      "Data columns (total 30 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Acidez                4998 non-null   float64\n",
      " 1   AcidoCitrico          4998 non-null   float64\n",
      " 2   Azucar                4998 non-null   float64\n",
      " 3   CloruroSodico         4998 non-null   float64\n",
      " 4   Densidad              4998 non-null   float64\n",
      " 5   pH                    4998 non-null   float64\n",
      " 6   Sulfatos              4998 non-null   float64\n",
      " 7   Alcohol               4998 non-null   float64\n",
      " 8   CalifProductor        4998 non-null   int64  \n",
      " 9   PrecioBotella         4998 non-null   float64\n",
      " 10  Etiqueta              4998 non-null   object \n",
      " 11  Clasificacion         4998 non-null   object \n",
      " 12  Region                4998 non-null   float64\n",
      " 13  prop_missings         4998 non-null   float64\n",
      " 14  Etiqueta2             4998 non-null   object \n",
      " 15  aleatorio             4998 non-null   float64\n",
      " 16  aleatorio2            4998 non-null   float64\n",
      " 17  Acidez_sqr            4998 non-null   float64\n",
      " 18  AcidoCitrico_exp      4998 non-null   float64\n",
      " 19  Azucar_sqr            4998 non-null   float64\n",
      " 20  CloruroSodico_log     4998 non-null   float64\n",
      " 21  Densidad_log          4998 non-null   float64\n",
      " 22  pH_log                4998 non-null   float64\n",
      " 23  Sulfatos_exp          4998 non-null   float64\n",
      " 24  Alcohol_raiz4         4998 non-null   float64\n",
      " 25  CalifProductor_raiz4  4998 non-null   float64\n",
      " 26  PrecioBotella_sqr     4998 non-null   float64\n",
      " 27  aleatorio_log         4998 non-null   float64\n",
      " 28  aleatorio2_log        4998 non-null   float64\n",
      " 29  Beneficio             4998 non-null   int64  \n",
      "dtypes: float64(25), int64(2), object(3)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Leer datos depurados datosvinoDep\n",
    "todo_cont = pd.read_csv('C:\\\\Users\\\\Guille\\\\Documents\\\\MineriaDatos_2022_23\\\\PARTE I_Depuracion y Regresiones\\\\Dia2_Regresion Lineal\\\\todo_cont_cor.csv', index_col=0)\n",
    "\n",
    "# Descriptivo de comprobación\n",
    "todo_cont.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354e999",
   "metadata": {},
   "source": [
    " ### Preparación de los datos\n",
    " \n",
    " Como siempre sacamos la variable objetivo para tenerla controlada y creamos el input. Como en esta ocasión vamos a trabajar más con el paradigma modelización mediante X,y, necesitaremos generar explítcitamente la matriz de diseño total con las categóricas extendidas en dummies y con constante. Vamos a hacerlo de forma manual. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c49027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>Acidez</th>\n",
       "      <th>AcidoCitrico</th>\n",
       "      <th>Azucar</th>\n",
       "      <th>CloruroSodico</th>\n",
       "      <th>Densidad</th>\n",
       "      <th>pH</th>\n",
       "      <th>Sulfatos</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>CalifProductor</th>\n",
       "      <th>...</th>\n",
       "      <th>Clasificacion_**</th>\n",
       "      <th>Clasificacion_***</th>\n",
       "      <th>Clasificacion_****</th>\n",
       "      <th>Clasificacion_Desc</th>\n",
       "      <th>Etiqueta_B</th>\n",
       "      <th>Etiqueta_M</th>\n",
       "      <th>Etiqueta_MB</th>\n",
       "      <th>Etiqueta_R</th>\n",
       "      <th>Region_2.0</th>\n",
       "      <th>Region_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>26.10</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>1.02792</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.70</td>\n",
       "      <td>15.4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.99518</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.48</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.22</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.03236</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.33</td>\n",
       "      <td>11.6</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.05</td>\n",
       "      <td>11.25</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.26</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.94724</td>\n",
       "      <td>3.09</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   const  Acidez  AcidoCitrico  Azucar  CloruroSodico  Densidad    pH  \\\n",
       "0    1.0    0.16         -0.81   26.10         -0.425   1.02792  3.38   \n",
       "1    1.0    2.64         -0.88   14.80          0.037   0.99518  3.12   \n",
       "3    1.0   -1.22          0.34    1.40          0.040   1.03236  3.20   \n",
       "4    1.0    0.27          1.05   11.25         -0.007   0.99620  4.93   \n",
       "5    1.0   -0.22          0.39    1.80         -0.277   0.94724  3.09   \n",
       "\n",
       "   Sulfatos  Alcohol  CalifProductor  ...  Clasificacion_**  \\\n",
       "0      0.70     15.4               2  ...                 0   \n",
       "1      0.48     22.0               3  ...                 0   \n",
       "3      0.33     11.6               2  ...                 0   \n",
       "4      0.26     15.0               1  ...                 0   \n",
       "5      0.75     12.6               3  ...                 0   \n",
       "\n",
       "   Clasificacion_*** Clasificacion_****  Clasificacion_Desc  Etiqueta_B  \\\n",
       "0                  1                  0                   0           0   \n",
       "1                  1                  0                   0           0   \n",
       "3                  1                  0                   0           1   \n",
       "4                  0                  0                   1           0   \n",
       "5                  0                  1                   0           0   \n",
       "\n",
       "   Etiqueta_M  Etiqueta_MB  Etiqueta_R  Region_2.0  Region_3.0  \n",
       "0           1            0           0           0           0  \n",
       "1           1            0           0           0           1  \n",
       "3           0            0           0           1           0  \n",
       "4           0            0           1           1           0  \n",
       "5           0            0           1           1           0  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varObjCont = todo_cont.Beneficio\n",
    "imput = todo_cont.drop(['Beneficio'],axis=1)\n",
    "\n",
    "# Craer matriz de diseño \n",
    "imput_dummy = pd.get_dummies(imput, columns=['Clasificacion', 'Etiqueta', 'Region'], drop_first=False)\n",
    "\n",
    "# Borramos los niveles que queramos como referencia (se incluirá su efecto implicito en las constante)\n",
    "imput_dummy.drop(['Etiqueta_MM','Clasificacion_*','Region_1.0'], axis=1, inplace=True)\n",
    "# Añadir constante\n",
    "imput_dummy=sm.add_constant(imput_dummy)\n",
    "\n",
    "imput_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b748cf8b",
   "metadata": {},
   "source": [
    "Tomamos las particiones de training y test desde la matriz de diseño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046a7bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (3998, 37) (3998,)\n",
      "Testing dataset shape: (1000, 37) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Función necesaria\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creamos 4 objetos: predictores para tr y tst y variable objetivo para tr y tst. \n",
    "X_train, X_test, y_train, y_test = train_test_split(imput_dummy, varObjCont, test_size=0.2, random_state=42)\n",
    "\n",
    "# Comprobamos dimensiones\n",
    "print('Training dataset shape:', X_train.shape, y_train.shape)\n",
    "print('Testing dataset shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a47f3",
   "metadata": {},
   "source": [
    "### Modelo con todos los efectos\n",
    "\n",
    "Ajsutamos un modelo con todos los efectos que, aunque inutil por sus múltiples problemas de colinealidad, sobreparametrización etc, nos sirve para controlar como está la cosa con las variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997ea2df",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Genero el training con la objetivo dentro \u001b[39;00m\n\u001b[0;32m      6\u001b[0m data_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mjoin(y_train)\n\u001b[1;32m----> 8\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m      9\u001b[0m res\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:890\u001b[0m, in \u001b[0;36mOLS.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are not supported in OLS and will be ignored\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn exception will be raised in the next version.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    889\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, ValueWarning)\n\u001b[1;32m--> 890\u001b[0m \u001b[38;5;28msuper\u001b[39m(OLS, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    891\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys:\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_keys\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:717\u001b[0m, in \u001b[0;36mWLS.__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     weights \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 717\u001b[0m \u001b[38;5;28msuper\u001b[39m(WLS, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    718\u001b[0m                           weights\u001b[38;5;241m=\u001b[39mweights, hasconst\u001b[38;5;241m=\u001b[39mhasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    719\u001b[0m nobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    720\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:191\u001b[0m, in \u001b[0;36mRegressionModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28msuper\u001b[39m(RegressionModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_attr\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpinv_wexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwendog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwexog\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:267\u001b[0m, in \u001b[0;36mLikelihoodModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:92\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m missing \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m hasconst \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhasconst\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_data(endog, exog, missing, hasconst,\n\u001b[0;32m     93\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mk_constant\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexog\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:132\u001b[0m, in \u001b[0;36mModel._handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 132\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data(endog, exog, missing, hasconst, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py:673\u001b[0m, in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m     exog \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    672\u001b[0m klass \u001b[38;5;241m=\u001b[39m handle_data_class_factory(endog, exog)\n\u001b[1;32m--> 673\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(endog, exog\u001b[38;5;241m=\u001b[39mexog, missing\u001b[38;5;241m=\u001b[39mmissing, hasconst\u001b[38;5;241m=\u001b[39mhasconst,\n\u001b[0;32m    674\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py:82\u001b[0m, in \u001b[0;36mModelData.__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_endog \u001b[38;5;241m=\u001b[39m endog\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_exog \u001b[38;5;241m=\u001b[39m exog\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendog, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexog \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_constant \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\statsmodels\\base\\data.py:507\u001b[0m, in \u001b[0;36mPandasData._convert_endog_exog\u001b[1;34m(self, endog, exog)\u001b[0m\n\u001b[0;32m    505\u001b[0m exog \u001b[38;5;241m=\u001b[39m exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(exog)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas data cast to numpy dtype of object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck input data with np.asarray(data).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(PandasData, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_convert_endog_exog(endog, exog)\n",
      "\u001b[1;31mValueError\u001b[0m: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data)."
     ]
    }
   ],
   "source": [
    "# Importamos la api para fórmulas (en concreto ols para regresión)\n",
    "from statsmodels.formula.api import ols \n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Genero el training con la objetivo dentro \n",
    "data_train = X_train.join(y_train)\n",
    "\n",
    "res = sm.OLS(np.asarray(y_train),X_train).fit()\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15515838",
   "metadata": {},
   "source": [
    "\n",
    "## Modelo manual ganador\n",
    "\n",
    "Rescatamos el modelo ganador en nuestro proceso de ajuste manual de modelos de regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "527d4fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Beneficio</td>    <th>  R-squared:         </th> <td>   0.434</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.433</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   348.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 12 Nov 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:21:55</td>     <th>  Log-Likelihood:    </th> <td> -32696.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4998</td>      <th>  AIC:               </th> <td>6.542e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4986</td>      <th>  BIC:               </th> <td>6.549e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>             <td>  643.8888</td> <td>   11.597</td> <td>   55.520</td> <td> 0.000</td> <td>  621.153</td> <td>  666.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Etiqueta[T.M]</th>         <td> -261.4073</td> <td>    7.246</td> <td>  -36.077</td> <td> 0.000</td> <td> -275.612</td> <td> -247.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Etiqueta[T.MB]</th>        <td>  109.7725</td> <td>   13.567</td> <td>    8.091</td> <td> 0.000</td> <td>   83.175</td> <td>  136.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Etiqueta[T.MM]</th>        <td> -384.7397</td> <td>   13.117</td> <td>  -29.331</td> <td> 0.000</td> <td> -410.455</td> <td> -359.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Etiqueta[T.R]</th>         <td> -129.0588</td> <td>    6.249</td> <td>  -20.654</td> <td> 0.000</td> <td> -141.309</td> <td> -116.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clasificacion[T.**]</th>   <td>   45.7919</td> <td>    6.381</td> <td>    7.176</td> <td> 0.000</td> <td>   33.282</td> <td>   58.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clasificacion[T.***]</th>  <td>  104.1102</td> <td>    7.326</td> <td>   14.212</td> <td> 0.000</td> <td>   89.749</td> <td>  118.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clasificacion[T.****]</th> <td>  188.2680</td> <td>   11.028</td> <td>   17.073</td> <td> 0.000</td> <td>  166.649</td> <td>  209.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clasificacion[T.Desc]</th> <td>  -21.2381</td> <td>    8.105</td> <td>   -2.621</td> <td> 0.009</td> <td>  -37.127</td> <td>   -5.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CalifProductor</th>        <td>   -8.4614</td> <td>    2.216</td> <td>   -3.818</td> <td> 0.000</td> <td>  -12.806</td> <td>   -4.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Acidez</th>                <td>   -7.4238</td> <td>    3.108</td> <td>   -2.389</td> <td> 0.017</td> <td>  -13.516</td> <td>   -1.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Alcohol</th>               <td>    4.0767</td> <td>    0.669</td> <td>    6.091</td> <td> 0.000</td> <td>    2.765</td> <td>    5.389</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>132.062</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 142.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.413</td>  <th>  Prob(JB):          </th> <td>1.30e-31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.044</td>  <th>  Cond. No.          </th> <td>    80.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Beneficio   R-squared:                       0.434\n",
       "Model:                            OLS   Adj. R-squared:                  0.433\n",
       "Method:                 Least Squares   F-statistic:                     348.2\n",
       "Date:                Sat, 12 Nov 2022   Prob (F-statistic):               0.00\n",
       "Time:                        12:21:55   Log-Likelihood:                -32696.\n",
       "No. Observations:                4998   AIC:                         6.542e+04\n",
       "Df Residuals:                    4986   BIC:                         6.549e+04\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=========================================================================================\n",
       "                            coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "Intercept               643.8888     11.597     55.520      0.000     621.153     666.625\n",
       "Etiqueta[T.M]          -261.4073      7.246    -36.077      0.000    -275.612    -247.202\n",
       "Etiqueta[T.MB]          109.7725     13.567      8.091      0.000      83.175     136.370\n",
       "Etiqueta[T.MM]         -384.7397     13.117    -29.331      0.000    -410.455    -359.024\n",
       "Etiqueta[T.R]          -129.0588      6.249    -20.654      0.000    -141.309    -116.808\n",
       "Clasificacion[T.**]      45.7919      6.381      7.176      0.000      33.282      58.302\n",
       "Clasificacion[T.***]    104.1102      7.326     14.212      0.000      89.749     118.472\n",
       "Clasificacion[T.****]   188.2680     11.028     17.073      0.000     166.649     209.887\n",
       "Clasificacion[T.Desc]   -21.2381      8.105     -2.621      0.009     -37.127      -5.350\n",
       "CalifProductor           -8.4614      2.216     -3.818      0.000     -12.806      -4.117\n",
       "Acidez                   -7.4238      3.108     -2.389      0.017     -13.516      -1.331\n",
       "Alcohol                   4.0767      0.669      6.091      0.000       2.765       5.389\n",
       "==============================================================================\n",
       "Omnibus:                      132.062   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              142.233\n",
       "Skew:                           0.413   Prob(JB):                     1.30e-31\n",
       "Kurtosis:                       3.044   Cond. No.                         80.0\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajusto regresión de ejemplo\n",
    "results = ols('Beneficio ~ Etiqueta + Clasificacion + CalifProductor + Acidez + Alcohol',data=todo_cont).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a758dfbc",
   "metadata": {},
   "source": [
    "## Selección automática de variables \n",
    "\n",
    "Vamos a probar ahora los métodos clásicos de selección de variables que, partiendo del modelo completo/nulo eliminarán/añadirán secuencialmente variables hasta un número indicado o bien hasta alcanzar el score mejor o de mayor parsimonia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97c61c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 684, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'MB'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 684, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'R'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m sfs_back \u001b[38;5;241m=\u001b[39m sfs(clf,k_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m'\u001b[39m,forward\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,floating\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr2\u001b[39m\u001b[38;5;124m'\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Perform SFFS\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m sfs_back \u001b[38;5;241m=\u001b[39m \u001b[43msfs_back\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#print(sfs1.subsets_)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(sfs_back\u001b[38;5;241m.\u001b[39mk_feature_names_)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlxtend\\feature_selection\\sequential_feature_selector.py:519\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    517\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(k_idx)\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 519\u001b[0m     k_idx, k_score \u001b[38;5;241m=\u001b[39m _calc_score(\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    521\u001b[0m         X_,\n\u001b[0;32m    522\u001b[0m         y,\n\u001b[0;32m    523\u001b[0m         k_idx,\n\u001b[0;32m    524\u001b[0m         groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    525\u001b[0m         feature_groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_groups_,\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params,\n\u001b[0;32m    527\u001b[0m     )\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubsets_[k] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_idx,\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m: k_score,\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnanmean(k_score),\n\u001b[0;32m    532\u001b[0m     }\n\u001b[0;32m    534\u001b[0m orig_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_ub))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlxtend\\feature_selection\\utilities.py:98\u001b[0m, in \u001b[0;36m_calc_score\u001b[1;34m(selector, X, y, indices, groups, feature_groups, **fit_params)\u001b[0m\n\u001b[0;32m     96\u001b[0m IDX \u001b[38;5;241m=\u001b[39m _merge_lists(feature_groups, indices)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selector\u001b[38;5;241m.\u001b[39mcv:\n\u001b[1;32m---> 98\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mest_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIDX\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     selector\u001b[38;5;241m.\u001b[39mest_\u001b[38;5;241m.\u001b[39mfit(X[:, IDX], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 684, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'MB'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 684, in fit\n    X, y = self._validate_data(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 596, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1074, in check_X_y\n    X = check_array(\n  File \"C:\\Users\\Guille\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 856, in check_array\n    array = np.asarray(array, order=order, dtype=dtype)\nValueError: could not convert string to float: 'R'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "\n",
    "clf = LinearRegression()\n",
    "\n",
    "# Build step forward feature selection\n",
    "sfs_back = sfs(clf,k_features = 'best',forward=False,floating=False, scoring='r2',cv=5)\n",
    "\n",
    "# Perform SFFS\n",
    "sfs_back = sfs_back.fit(X_train, y_train)\n",
    "\n",
    "#print(sfs1.subsets_)\n",
    "\n",
    "print(sfs_back.k_feature_names_)\n",
    "\n",
    "sfs_back.k_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eec2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(sfs_back.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d332dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection\n",
    "sfs_forw = sfs(clf, \n",
    "          k_features='parsimonious', \n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='r2',\n",
    "          cv=4)\n",
    "\n",
    "sfs_forw = sfs_forw.fit(X_train, y_train)\n",
    "\n",
    "print('\\nSequential Backward Selection:')\n",
    "print(sfs_forw.k_feature_names_)\n",
    "print('CV Score:')\n",
    "print(sfs_forw.k_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso forward\n",
    "pd.DataFrame.from_dict(sfs_forw.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705cf2b8",
   "metadata": {},
   "source": [
    "### Visualicación del proceso de selección de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7050ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig1 = plot_sfs(sfs_forw.get_metric_dict(), kind='std_dev')\n",
    "\n",
    "#plt.ylim([0.8, 1])\n",
    "plt.title('Sequential Forward Selection (w. StdDev)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946db47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection\n",
    "sfs_12 = sfs(clf, \n",
    "          k_features= 12, \n",
    "          forward=False, \n",
    "          floating=True, \n",
    "          scoring='r2',\n",
    "          cv=4)\n",
    "\n",
    "sfs_12 = sfs_12.fit(X_train.drop(['aleatorio2_log'],axis=1), y_train)\n",
    "\n",
    "print('\\nSequential Forward Selection (k=12):')\n",
    "print(sfs_12.k_feature_names_)\n",
    "print('CV Score:')\n",
    "print(sfs_12.k_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb47081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection\n",
    "sfs_10 = sfs(clf, \n",
    "          k_features= 10, \n",
    "          forward=False, \n",
    "          floating=True, \n",
    "          scoring='r2',\n",
    "          cv=4)\n",
    "\n",
    "sfs_10 = sfs_10.fit(X_train.drop(['aleatorio2_log'],axis=1), y_train)\n",
    "\n",
    "print('\\nSequential Forward Selection (k=10):')\n",
    "print(sfs_10.k_feature_names_)\n",
    "print('CV Score:')\n",
    "print(sfs_10.k_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc5988",
   "metadata": {},
   "source": [
    "## Comparación por validación cruzada\n",
    "\n",
    "Comparamos el rendimiento de los modelos bajo el esquema de validación cruzada repetida creando una función similar a la que ya teníamos pero que, en esta ocasión trabaja sobre objetos de salida de los métodos de selección de variables de tal forma que en base a estos se seleccione el input adecuado y se ajuste el modelo lineal con las varibales seleccionadas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model =LinearRegression()\n",
    "\n",
    "# Función para comparación por validación cruzada\n",
    "def cross_val (sfs, data, y, seed=12345):\n",
    "        \n",
    "        X = sfs\n",
    "\n",
    "        if not isinstance(sfs,pd.DataFrame):\n",
    "            X = sfs.transform(data)\n",
    "\n",
    "        # Establecemos esquema de validación fijando random_state (reproducibilidad)\n",
    "        cv = RepeatedKFold(n_splits=5, n_repeats=20, random_state=seed)\n",
    "\n",
    "        # Obtenemos los resultados de R2 para cada partición tr-tst\n",
    "        scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1)\n",
    "\n",
    "        # Sesgo y varianza\n",
    "        print('Coeficiente de determinación R2: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "       # sns.violinplot(y=scores,palette='viridis')\n",
    "\n",
    "        return(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso par aun modelo\n",
    "# cross_val(sfs_back,imput_dummy,varObjCont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos lista de fórmulas   \n",
    "list_sfs = [sfs_back,sfs_forw,sfs_12,sfs_10]\n",
    "list_sfs\n",
    "\n",
    "# Aplicamos a toda la lista la función creada (devuelve un dataframe pero está transpuesto)\n",
    "list_res = pd.DataFrame(map(lambda x: cross_val(x,imput_dummy,varObjCont, seed=2022),list_sfs))\n",
    "\n",
    "# Trasnponer dataframe y pasar de wide a long (creando un factor variable con el nombre de cada fórmula de la lista[0,1,2,3])\n",
    "results = list_res.T.melt()\n",
    "results.columns = ['Modelo','R2']\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18540a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot paralelo para comparar\n",
    "sns.boxplot(x='Modelo',y='R2',data=results,palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a611f",
   "metadata": {},
   "source": [
    "## Selección de variables con interacciones\n",
    "\n",
    "Vamos ahora a considerar los efectos de interacción de orden 2 entre las variables para valorar si pueden aportar capacidad predictiva al modelo. \n",
    "\n",
    "Generaremos el dataset con las interacciones de todas las variables y posteriormente pasaremos los métodos de selección para hacer una criba de efectos interesantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e44ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_col = ['const', 'Acidez', 'CalifProductor', \n",
    "        'Acidez_sqr', 'Alcohol_raiz4', 'Clasificacion_**', 'Clasificacion_***', 'Clasificacion_****', \n",
    "        'Clasificacion_Desc', 'Etiqueta_B', 'Etiqueta_M', 'Etiqueta_MB', \n",
    "        'Etiqueta_R', 'Region_2.0']\n",
    "\n",
    "imput_red = imput_dummy[sel_col]\n",
    "X_train_red = X_train[sel_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\n",
    "\n",
    "# Create interaction terms (interaction of each regressor pair + polynomial)\n",
    "#Interaction terms need to be created in both the test and train datasets\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "interaction\n",
    "\n",
    "#traning\n",
    "imput_inter = pd.DataFrame(interaction.fit_transform(imput_red), columns=interaction.get_feature_names_out(input_features=imput_red.columns))\n",
    "X_inter = pd.DataFrame(interaction.fit_transform(X_train_red), columns=interaction.get_feature_names_out(input_features=X_train_red.columns))\n",
    "X_inter.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2350e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar columnas constantes (interacciones sin sentido)\n",
    "X_inter = X_inter.loc[:, X_inter.var() != 0.0]\n",
    "\n",
    "X_inter.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752aa092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection\n",
    "sfs_forw_int_10 = sfs(clf, \n",
    "          k_features=10, \n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='r2',\n",
    "          cv=4)\n",
    "\n",
    "sfs_forw_int_10 = sfs_forw_int_10.fit(X_inter, y_train)\n",
    "\n",
    "print('\\nSequential Backward Selection:')\n",
    "print(sfs_forw_int_10.k_feature_names_)\n",
    "print('CV Score:')\n",
    "print(sfs_forw_int_10.k_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc48c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential Forward Selection\n",
    "sfs_forw_int_best = sfs(clf, \n",
    "          k_features='best', \n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='r2',\n",
    "          cv=4)\n",
    "\n",
    "sfs_forw_int_best = sfs_forw_int_best.fit(X_inter, y_train)\n",
    "\n",
    "print('\\nSequential Backward Selection:')\n",
    "print(sfs_forw_int_best.k_feature_names_)\n",
    "print('CV Score:')\n",
    "print(sfs_forw_int_best.k_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8754711",
   "metadata": {},
   "source": [
    "## Selección de variables por LASSO\n",
    "\n",
    "Exploramos la selección de variables por modelo laso con criterios AIC o BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LassoLarsIC(criterion='bic', normalize=False)\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "print(reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c7883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_feats = X_train[X_train.columns[(reg.coef_ != 0).ravel().tolist()]]\n",
    "selec_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7a5a6",
   "metadata": {},
   "source": [
    "Lasso con interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43df51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_int = linear_model.LassoLarsIC(criterion='bic', normalize=False)\n",
    "\n",
    "lasso_int.fit(X_inter, y_train)\n",
    "\n",
    "print(lasso_int.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "selec_feats_int = X_inter[X_inter.columns[(lasso_int.coef_ != 0).ravel().tolist()]]\n",
    "selec_feats_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b13d368",
   "metadata": {},
   "source": [
    "### Validación Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sfs = [sfs_forw_int_10,sfs_forw_int_best,selec_feats,selec_feats_int]\n",
    "list_sfs\n",
    "\n",
    "data = imput_dummy.join(imput_inter,lsuffix=\"_left\")\n",
    "\n",
    "# Aplicamos a toda la lista la función creada (devuelve un dataframe pero está transpuesto)\n",
    "list_res = pd.DataFrame(map(lambda x: cross_val(x,X_inter,y_train, seed=2022),list_sfs))\n",
    "\n",
    "# Trasnponer dataframe y pasar de wide a long (creando un factor variable con el nombre de cada fórmula de la lista[0,1,2,3])\n",
    "results = list_res.T.melt()\n",
    "results.columns = ['Modelo','R2']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot paralelo para comparar\n",
    "sns.boxplot(x='Modelo',y='R2',data=results,palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0ad63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdbc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad53e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9d3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
